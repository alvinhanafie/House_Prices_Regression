{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Advanced House Prices Regression\n\nObjective: to predict the House Prices based on relevant features.\n\nSupervised Learning: Regression","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade scikit-learn","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:53:12.221535Z","iopub.execute_input":"2024-06-21T07:53:12.222072Z","iopub.status.idle":"2024-06-21T07:53:12.227631Z","shell.execute_reply.started":"2024-06-21T07:53:12.222028Z","shell.execute_reply":"2024-06-21T07:53:12.226267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Library\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, root_mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import StandardScaler\nfrom category_encoders import TargetEncoder\n\nfrom sklearn.pipeline import Pipeline\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom sklearn.metrics import mutual_info_score \nfrom sklearn.impute import SimpleImputer\nimport scipy.stats as stats\nfrom sklearn.model_selection import cross_val_score\n\nimport optuna\nimport optuna.visualization as vis\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set the maximum number of columns and rows to display to a large number\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","metadata":{"id":"F3gKXXAMVkAk","execution":{"iopub.status.busy":"2024-06-21T07:53:12.229789Z","iopub.execute_input":"2024-06-21T07:53:12.230190Z","iopub.status.idle":"2024-06-21T07:53:12.242705Z","shell.execute_reply.started":"2024-06-21T07:53:12.230156Z","shell.execute_reply":"2024-06-21T07:53:12.241420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# graphic settings\n\nsns.set_theme(rc={'figure.figsize':(20.7, 18.27)})\nsns.set_style(\"whitegrid\")\nsns.color_palette(\"dark\")\nplt.style.use(\"fivethirtyeight\")","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:53:12.244215Z","iopub.execute_input":"2024-06-21T07:53:12.244787Z","iopub.status.idle":"2024-06-21T07:53:12.259691Z","shell.execute_reply.started":"2024-06-21T07:53:12.244743Z","shell.execute_reply":"2024-06-21T07:53:12.258442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Dataset\n\ndf = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")","metadata":{"id":"xT9jeV2_Xtdj","execution":{"iopub.status.busy":"2024-06-21T07:53:12.262914Z","iopub.execute_input":"2024-06-21T07:53:12.263420Z","iopub.status.idle":"2024-06-21T07:53:12.312912Z","shell.execute_reply.started":"2024-06-21T07:53:12.263376Z","shell.execute_reply":"2024-06-21T07:53:12.311577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset Information Overview","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"id":"kmZEIo6bYJrX","outputId":"ddac4062-52b3-499a-9963-d6bec041cad2","execution":{"iopub.status.busy":"2024-06-21T07:53:12.315991Z","iopub.execute_input":"2024-06-21T07:53:12.316657Z","iopub.status.idle":"2024-06-21T07:53:12.416677Z","shell.execute_reply.started":"2024-06-21T07:53:12.316594Z","shell.execute_reply":"2024-06-21T07:53:12.415407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:53:12.419581Z","iopub.execute_input":"2024-06-21T07:53:12.420467Z","iopub.status.idle":"2024-06-21T07:53:12.456604Z","shell.execute_reply.started":"2024-06-21T07:53:12.420402Z","shell.execute_reply":"2024-06-21T07:53:12.454862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some columns have missing values. We will deal with this later in data preprocessing. ","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:53:12.458626Z","iopub.execute_input":"2024-06-21T07:53:12.459504Z","iopub.status.idle":"2024-06-21T07:53:12.616893Z","shell.execute_reply.started":"2024-06-21T07:53:12.459433Z","shell.execute_reply":"2024-06-21T07:53:12.614989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(include='object')","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:53:12.618696Z","iopub.execute_input":"2024-06-21T07:53:12.619199Z","iopub.status.idle":"2024-06-21T07:53:12.752902Z","shell.execute_reply.started":"2024-06-21T07:53:12.619156Z","shell.execute_reply":"2024-06-21T07:53:12.751297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some features such as Street and Utilities dominated the class, we will check the variance in feature selection step. ","metadata":{}},{"cell_type":"markdown","source":"### Train Test Data Split","metadata":{}},{"cell_type":"code","source":"# drop Id column because it is not used\n\ndf.drop('Id', axis = 1, inplace=True)\n\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:53:12.757216Z","iopub.execute_input":"2024-06-21T07:53:12.757638Z","iopub.status.idle":"2024-06-21T07:53:12.775422Z","shell.execute_reply.started":"2024-06-21T07:53:12.757606Z","shell.execute_reply":"2024-06-21T07:53:12.773914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# separate variables into 3 categories based on data types\n# numerical features with unique values lower than 20 will be categorized as discrete variables. \n\ncategorical = [col for col in df_train.columns if df[col].dtype not in ['float64','int64']]\ndiscrete = [col for col in df_train.columns if df[col].nunique() < 20 and col not in categorical]\nnumerical = [col for col in df_train.columns if col not in categorical + discrete + ['SalePrice']]\n\ntarget = 'SalePrice' # for EDA purpose","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:53:12.777056Z","iopub.execute_input":"2024-06-21T07:53:12.777427Z","iopub.status.idle":"2024-06-21T07:53:12.809314Z","shell.execute_reply.started":"2024-06-21T07:53:12.777395Z","shell.execute_reply":"2024-06-21T07:53:12.807795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will perform Exploratory Data Analysis for each features data types: categorical, discrete, and numerical. ","metadata":{}},{"cell_type":"markdown","source":"### Categorical Variables","metadata":{}},{"cell_type":"code","source":"df_train_categorical = df_train[categorical]","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:53:12.810982Z","iopub.execute_input":"2024-06-21T07:53:12.811527Z","iopub.status.idle":"2024-06-21T07:53:12.820451Z","shell.execute_reply.started":"2024-06-21T07:53:12.811491Z","shell.execute_reply":"2024-06-21T07:53:12.818859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[categorical].describe()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:53:12.822255Z","iopub.execute_input":"2024-06-21T07:53:12.822733Z","iopub.status.idle":"2024-06-21T07:53:12.938401Z","shell.execute_reply.started":"2024-06-21T07:53:12.822685Z","shell.execute_reply":"2024-06-21T07:53:12.937148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data Distribution for Categorical Variables","metadata":{}},{"cell_type":"code","source":"# distribution check in categorical variables\n\nfor i in range(0, len(df_train_categorical.columns)):\n    plt.subplot(8, 6, i+1)\n    sns.countplot(x=df_train_categorical[categorical[i]])\n    plt.title(categorical[i])\n    x=plt.xticks(rotation=90)\n    plt.suptitle('Data Distribution in Categorical Variables', fontsize = 20)\n    plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:53:12.940209Z","iopub.execute_input":"2024-06-21T07:53:12.940664Z","iopub.status.idle":"2024-06-21T07:53:56.757408Z","shell.execute_reply.started":"2024-06-21T07:53:12.940625Z","shell.execute_reply":"2024-06-21T07:53:56.756147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some of features have a dominant unique value, such as Street, CentralAir, and Electrical. Neighborhood have the highest number of unique values. ","metadata":{}},{"cell_type":"markdown","source":"#### Relation between Categorical Variables and Target","metadata":{}},{"cell_type":"code","source":"for i in range(0, len(df_train_categorical.columns)):\n    plt.subplot(8, 6, i+1)\n    sns.boxplot(x=df_train_categorical[categorical[i]], y = target, data = df_train)\n    plt.title(categorical[i])\n    x=plt.xticks(rotation=90)\n    plt.suptitle('Relation between Categorical Variables and Target', fontsize = 20)\n    plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:53:56.758778Z","iopub.execute_input":"2024-06-21T07:53:56.759177Z","iopub.status.idle":"2024-06-21T07:54:46.690189Z","shell.execute_reply.started":"2024-06-21T07:53:56.759145Z","shell.execute_reply":"2024-06-21T07:54:46.688939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are some features such as ExterQual, BsmtQual, and MiscFeature have significant difference of Sale Price between its category. ","metadata":{}},{"cell_type":"markdown","source":"### Discrete Variables","metadata":{}},{"cell_type":"code","source":"df_train_discrete = df_train[discrete]","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:54:46.691782Z","iopub.execute_input":"2024-06-21T07:54:46.692219Z","iopub.status.idle":"2024-06-21T07:54:46.698670Z","shell.execute_reply.started":"2024-06-21T07:54:46.692187Z","shell.execute_reply":"2024-06-21T07:54:46.697394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data Distribution for Discrete Features","metadata":{}},{"cell_type":"code","source":"for i in range(0, len(df_train_discrete.columns)):\n    plt.subplot(4, 4, i+1)\n    sns.countplot(x=df_train_discrete[discrete[i]])\n    plt.title(discrete[i])\n    plt.suptitle('Data Distribution in Discrete Variables', fontsize = 20)\n    plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:54:46.700396Z","iopub.execute_input":"2024-06-21T07:54:46.700751Z","iopub.status.idle":"2024-06-21T07:54:57.692354Z","shell.execute_reply.started":"2024-06-21T07:54:46.700713Z","shell.execute_reply":"2024-06-21T07:54:57.690957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"BsmtHalfBath, KitchenAbvGr, and PoolArea features have imbalance distribution between their unique values. ","metadata":{}},{"cell_type":"markdown","source":"#### Relation between Discrete Features and Target","metadata":{}},{"cell_type":"code","source":"for i in range(0, len(df_train_discrete.columns)):\n    plt.subplot(8, 6, i+1)\n    sns.boxplot(x=df_train_discrete[discrete[i]], y = target, data = df_train)\n    plt.title(discrete[i])\n    x=plt.xticks(rotation=90)\n    plt.suptitle('Relation between Discrete Variables and Target', fontsize = 20)\n    plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:54:57.694230Z","iopub.execute_input":"2024-06-21T07:54:57.694601Z","iopub.status.idle":"2024-06-21T07:55:08.474812Z","shell.execute_reply.started":"2024-06-21T07:54:57.694573Z","shell.execute_reply":"2024-06-21T07:55:08.473648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some features such as OverallQual, BsmtFullBath, and TotRmsAbvGrd have positive correlation to Sale Price. ","metadata":{}},{"cell_type":"markdown","source":"### Numerical Variables","metadata":{}},{"cell_type":"code","source":"df_train_numerical = df_train[numerical]","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:08.476666Z","iopub.execute_input":"2024-06-21T07:55:08.477066Z","iopub.status.idle":"2024-06-21T07:55:08.483954Z","shell.execute_reply.started":"2024-06-21T07:55:08.477028Z","shell.execute_reply":"2024-06-21T07:55:08.482576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data Distribution for Numerical Features","metadata":{}},{"cell_type":"code","source":"for i in range(0, len(df_train_numerical.columns)):\n    plt.subplot(5, 5, i+1)\n    sns.histplot(df_train_numerical[numerical[i]], kde = True)\n    plt.title(numerical[i])\n    plt.suptitle('Data Distribution in Numerical Variables', fontsize = 20)\n    plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:08.485680Z","iopub.execute_input":"2024-06-21T07:55:08.486561Z","iopub.status.idle":"2024-06-21T07:55:31.291770Z","shell.execute_reply.started":"2024-06-21T07:55:08.486525Z","shell.execute_reply":"2024-06-21T07:55:31.290573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Almost all features are positively skewed, concentrated around value of 0. ","metadata":{}},{"cell_type":"markdown","source":"#### Relation between Numerical Features and Target","metadata":{}},{"cell_type":"code","source":"for i in range(0, len(numerical)):\n    plt.subplot(5, 5, i+1)\n    sns.regplot(x=df_train_numerical[numerical[i]], y=target, data = df_train, scatter_kws={'s':10}, line_kws={'color':'red'})\n    plt.xlabel(numerical[i])\n    plt.ylabel(target)\n    plt.title(f'{numerical[i]} vs {target}')\n    plt.suptitle('Relation between Numerical Variables and Target', fontsize = 20)\n    plt.tight_layout()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:31.293511Z","iopub.execute_input":"2024-06-21T07:55:31.293942Z","iopub.status.idle":"2024-06-21T07:55:53.977280Z","shell.execute_reply.started":"2024-06-21T07:55:31.293908Z","shell.execute_reply":"2024-06-21T07:55:53.976069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some features like LotFrontage, TotalBsmtSF, and GrLivArea have highly positive correlation to Sale Price. ","metadata":{}},{"cell_type":"markdown","source":"### Target Distribution","metadata":{}},{"cell_type":"code","source":"sns.histplot(x=df_train[target], kde=True)\nplt.title('SalePrice')\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:53.979195Z","iopub.execute_input":"2024-06-21T07:55:53.979589Z","iopub.status.idle":"2024-06-21T07:55:54.991191Z","shell.execute_reply.started":"2024-06-21T07:55:53.979556Z","shell.execute_reply":"2024-06-21T07:55:54.989927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sale Price have tendency to positively skewed, with the center around 100000 - 200000.","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### Missing Value Handling","metadata":{}},{"cell_type":"code","source":"#checking missing values in percentage\n\nmissing = df_train.isnull().sum()\nmissing = missing[missing > 0]\nmissing = round(missing/len(df_train)*100,2)\nmissing.sort_values(ascending = False, inplace=True)\nmissing","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:54.993020Z","iopub.execute_input":"2024-06-21T07:55:54.993386Z","iopub.status.idle":"2024-06-21T07:55:55.013818Z","shell.execute_reply.started":"2024-06-21T07:55:54.993354Z","shell.execute_reply":"2024-06-21T07:55:55.012438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will drop columns that have more than 20% missing value.\n\nPoolQC, MiscFeature, Alley, Fence, MasVnrType, and FireplaceQu will be dropped because more than 20% data in those columns are missing. ","metadata":{}},{"cell_type":"code","source":"df_train = df_train.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'MasVnrType', 'FireplaceQu'], axis = 1)\ndf_test = df_test.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'MasVnrType', 'FireplaceQu'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:55.015378Z","iopub.execute_input":"2024-06-21T07:55:55.015828Z","iopub.status.idle":"2024-06-21T07:55:55.028095Z","shell.execute_reply.started":"2024-06-21T07:55:55.015778Z","shell.execute_reply":"2024-06-21T07:55:55.026726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adjust variables after feature drop\n\nremove_features = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'MasVnrType', 'FireplaceQu']\n\ncategorical = [item for item in categorical if item not in remove_features]","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:55.041437Z","iopub.execute_input":"2024-06-21T07:55:55.041897Z","iopub.status.idle":"2024-06-21T07:55:55.047831Z","shell.execute_reply.started":"2024-06-21T07:55:55.041865Z","shell.execute_reply":"2024-06-21T07:55:55.046621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even though the initial checking of missing value shows no missing values in all features, we will check thoroughly whether the dataset is not have \"hidden\" missing value, such as unknown or invalid data, by checking them through value counts for each features.","metadata":{}},{"cell_type":"code","source":"for col in df_train.columns:\n    print(f\"============= {col} =================\")\n    display(df_train[col].value_counts())\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:55.049529Z","iopub.execute_input":"2024-06-21T07:55:55.049994Z","iopub.status.idle":"2024-06-21T07:55:55.551086Z","shell.execute_reply.started":"2024-06-21T07:55:55.049949Z","shell.execute_reply":"2024-06-21T07:55:55.549900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After checking all unique values, there is no more missing or invalid values. ","metadata":{}},{"cell_type":"markdown","source":"### Impute Missing Value","metadata":{}},{"cell_type":"markdown","source":"Numerical and discrete features missing value will be imputed by their median, while categorical features will be imputed by their mode. ","metadata":{}},{"cell_type":"code","source":"impute_numerical = SimpleImputer(strategy = 'median')\ndf_train[numerical] = impute_numerical.fit_transform(df_train[numerical])\ndf_test[numerical] = impute_numerical.transform(df_test[numerical])\n\nimpute_discrete = SimpleImputer(strategy = 'median')\ndf_train[discrete] = impute_discrete.fit_transform(df_train[discrete])\ndf_test[discrete] = impute_discrete.transform(df_test[discrete])\n\nimpute_categorical = SimpleImputer(strategy = 'most_frequent')\ndf_train[categorical] = impute_categorical.fit_transform(df_train[categorical])\ndf_test[categorical] = impute_categorical.transform(df_test[categorical])","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:55.552628Z","iopub.execute_input":"2024-06-21T07:55:55.553075Z","iopub.status.idle":"2024-06-21T07:55:56.108538Z","shell.execute_reply.started":"2024-06-21T07:55:55.552993Z","shell.execute_reply":"2024-06-21T07:55:56.107257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check Duplicate Data","metadata":{}},{"cell_type":"code","source":"df_train.duplicated().any()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:56.110404Z","iopub.execute_input":"2024-06-21T07:55:56.110801Z","iopub.status.idle":"2024-06-21T07:55:56.138340Z","shell.execute_reply.started":"2024-06-21T07:55:56.110769Z","shell.execute_reply":"2024-06-21T07:55:56.136731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No duplicate values found in the dataset. ","metadata":{}},{"cell_type":"markdown","source":"## Feature Selection","metadata":{}},{"cell_type":"markdown","source":"We will do feature selection for each data types.","metadata":{}},{"cell_type":"markdown","source":"#### Categorical \n\nCategorical Features will be selected by their mutual information scores. We will use features that have cumulative importance threshold of 80%. ","metadata":{}},{"cell_type":"code","source":"def mutual_information_score(series):\n    return mutual_info_score(series,df_train.SalePrice)\n\ndf_mi = df_train[categorical].apply(mutual_information_score)\ndf_mi = df_mi.sort_values(ascending=False).to_frame(name='MI')\nmi_scores = pd.Series(df_mi['MI'], index=categorical)\nmi_scores.sort_values(ascending=False, inplace=True)\n\ndf_mi.style.background_gradient(low=0.7, high=1.0,cmap='YlOrRd')","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:56.139819Z","iopub.execute_input":"2024-06-21T07:55:56.140306Z","iopub.status.idle":"2024-06-21T07:55:56.324154Z","shell.execute_reply.started":"2024-06-21T07:55:56.140263Z","shell.execute_reply":"2024-06-21T07:55:56.322965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort scores and calculate cumulative importance\ncumulative_importance = mi_scores.cumsum() / mi_scores.sum()\n\n# Select features until cumulative importance threshold\ncumulative_threshold = 0.8\nselected_features = mi_scores[cumulative_importance <= cumulative_threshold].index\n\nprint(\"Selected features on categorical features based on cumulative importance:\\n\")\nprint(selected_features)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:56.325597Z","iopub.execute_input":"2024-06-21T07:55:56.325998Z","iopub.status.idle":"2024-06-21T07:55:56.335100Z","shell.execute_reply.started":"2024-06-21T07:55:56.325966Z","shell.execute_reply":"2024-06-21T07:55:56.333933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_categorical = df_train[selected_features]\ndf_train_categorical.columns","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:56.336883Z","iopub.execute_input":"2024-06-21T07:55:56.337988Z","iopub.status.idle":"2024-06-21T07:55:56.356415Z","shell.execute_reply.started":"2024-06-21T07:55:56.337944Z","shell.execute_reply":"2024-06-21T07:55:56.354611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Discrete \n\nDiscrete features will be selected based on variance. Features with low variance will be dropped, since it will not have significant relevance to modeling. Features will low variance will be checked through heatmap correlation, to analyze their correlation with target variable. ","metadata":{}},{"cell_type":"code","source":"def highest_unique_value_and_percentage(column):\n    value_counts = column.value_counts()\n    highest_value = value_counts.idxmax()\n    highest_count = value_counts.max()\n    total_count = len(column)\n    percentage = (highest_count / total_count) * 100\n    return highest_value, percentage\n\n# Apply the function to each categorical column\nhighest_values_and_percentages = {col: highest_unique_value_and_percentage(df_train_discrete[col]) for col in discrete}\n\nfor col, (value, percentage) in highest_values_and_percentages.items():\n    print(f\"Column: {col}, Highest Value: {value}, Percentage: {percentage:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:56.358216Z","iopub.execute_input":"2024-06-21T07:55:56.358643Z","iopub.status.idle":"2024-06-21T07:55:56.376771Z","shell.execute_reply.started":"2024-06-21T07:55:56.358611Z","shell.execute_reply":"2024-06-21T07:55:56.375507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature variance is represented by unique values percentage. BsmtHalfBath, PoolArea, KitchenAbvGr have dominant unique value (>90%) for 1 category, that related to low variance. ","metadata":{}},{"cell_type":"markdown","source":"Correlation Heatmap","metadata":{}},{"cell_type":"code","source":"# heatmap correlation\n\ncorr = pd.concat([df_train_discrete, df_train['SalePrice']], axis=1).corr(method='spearman')\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nsns.heatmap(corr, cmap = 'Blues', annot=True, mask = mask)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:56.378354Z","iopub.execute_input":"2024-06-21T07:55:56.378721Z","iopub.status.idle":"2024-06-21T07:55:57.646222Z","shell.execute_reply.started":"2024-06-21T07:55:56.378679Z","shell.execute_reply":"2024-06-21T07:55:57.644939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After checking correlation between features and target, KitchenAbvGr has weak relationship, while BsmtHalfBath and PoolArea almost have no correlation at all. These two features will be dropped. Additionally, features MSSubClass, MoSold, YrSold also have almost no correlation with target, hence all these features will be dropped. The other features do not have multicolinearity to each other. ","metadata":{}},{"cell_type":"code","source":"remove_discrete = ['MSSubClass', 'BsmtHalfBath', 'PoolArea', 'MoSold', 'YrSold']\n\ndf_train_discrete = df_train_discrete.drop(remove_discrete, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:57.647623Z","iopub.execute_input":"2024-06-21T07:55:57.648029Z","iopub.status.idle":"2024-06-21T07:55:57.655045Z","shell.execute_reply.started":"2024-06-21T07:55:57.647996Z","shell.execute_reply":"2024-06-21T07:55:57.653780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discrete = [item for item in discrete if item not in remove_discrete]\n\ndf_train_discrete.columns","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:57.657090Z","iopub.execute_input":"2024-06-21T07:55:57.658417Z","iopub.status.idle":"2024-06-21T07:55:57.671514Z","shell.execute_reply.started":"2024-06-21T07:55:57.658355Z","shell.execute_reply":"2024-06-21T07:55:57.670173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Numerical\n\nNumerical Features will be selected based on variance inflation factor (VIF) and correlation.","metadata":{}},{"cell_type":"code","source":"def calculate_vif(df):\n    vif_data = pd.DataFrame()\n    vif_data[\"Feature\"] = df.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n    return vif_data","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:57.673337Z","iopub.execute_input":"2024-06-21T07:55:57.673764Z","iopub.status.idle":"2024-06-21T07:55:57.681632Z","shell.execute_reply.started":"2024-06-21T07:55:57.673727Z","shell.execute_reply":"2024-06-21T07:55:57.679931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_numerical = df_train[numerical]","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:57.683344Z","iopub.execute_input":"2024-06-21T07:55:57.683753Z","iopub.status.idle":"2024-06-21T07:55:57.697894Z","shell.execute_reply.started":"2024-06-21T07:55:57.683722Z","shell.execute_reply":"2024-06-21T07:55:57.696502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate initial VIF\n\nvif_data = calculate_vif(df_train_numerical)\nprint(f'{vif_data}\\n')\n\nfeatures_removed = []\n\n# Iteratively remove features with VIF > 10\n# Feature that has highest VIF will be dropped one by one per iteration\n\nwhile vif_data['VIF'].max() > 10:\n    max_vif_feature = vif_data.loc[vif_data['VIF'].idxmax(), 'Feature']\n    print(f\"Removing feature with high VIF: {max_vif_feature}\")\n    features_removed.append(max_vif_feature)\n    df_train_numerical = df_train_numerical.drop(columns=[max_vif_feature])\n    vif_data = calculate_vif(df_train_numerical)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:57.700454Z","iopub.execute_input":"2024-06-21T07:55:57.700921Z","iopub.status.idle":"2024-06-21T07:55:58.858936Z","shell.execute_reply.started":"2024-06-21T07:55:57.700873Z","shell.execute_reply":"2024-06-21T07:55:58.857158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Remaining features after VIF reduction:\\n\")\nprint(df_train_numerical.columns)\nprint(f'\\nFinal VIF scores:\\n{vif_data}')","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:58.860985Z","iopub.execute_input":"2024-06-21T07:55:58.861664Z","iopub.status.idle":"2024-06-21T07:55:58.877594Z","shell.execute_reply.started":"2024-06-21T07:55:58.861620Z","shell.execute_reply":"2024-06-21T07:55:58.875998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Correlation Heatmap","metadata":{}},{"cell_type":"code","source":"# heatmap correlation\n\ncorr = pd.concat([df_train_numerical, df_train['SalePrice']], axis=1).corr()\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nsns.heatmap(corr, cmap = 'Blues', annot=True, mask = mask)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:55:58.879302Z","iopub.execute_input":"2024-06-21T07:55:58.880482Z","iopub.status.idle":"2024-06-21T07:56:00.078332Z","shell.execute_reply.started":"2024-06-21T07:55:58.880432Z","shell.execute_reply":"2024-06-21T07:56:00.076528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no features that have high correlation to each other. GarageArea and MasVnrArea have positive moderate correlation with SalePrice.","metadata":{}},{"cell_type":"markdown","source":"## Outlier Handling","metadata":{}},{"cell_type":"code","source":"# function to check histogram, distribution plot, and boxplot for each features\n\ndef check_plot(df, variable):\n    # check distribution plot from variable in df.     \n\n    # figure size and title\n    plt.figure(figsize=(16, 4))\n    plt.suptitle(f' Outlier Analysis for {variable} feature', fontsize=16, y=1.05)\n\n    # histogram\n    plt.subplot(1, 3, 1)\n    sns.histplot(df[variable], bins = 30)\n    plt.title('Histogram')\n\n    # distribution (Q-Q) plot  \n    plt.subplot(1, 3, 2)\n    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n    plt.ylabel('Variable quantiles')\n\n    # box plot\n    plt.subplot(1, 3, 3)\n    sns.boxplot(y=df[variable])\n    plt.title('Boxplot')\n\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:00.080219Z","iopub.execute_input":"2024-06-21T07:56:00.080708Z","iopub.status.idle":"2024-06-21T07:56:00.092190Z","shell.execute_reply.started":"2024-06-21T07:56:00.080665Z","shell.execute_reply":"2024-06-21T07:56:00.090917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot looping for distribution analysis each features\n\nfor col in df_train_numerical.columns:\n    check_plot(df_train_numerical, col)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:00.093725Z","iopub.execute_input":"2024-06-21T07:56:00.094147Z","iopub.status.idle":"2024-06-21T07:56:13.521702Z","shell.execute_reply.started":"2024-06-21T07:56:00.094115Z","shell.execute_reply":"2024-06-21T07:56:13.520405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Almost all features have global and local outliers. IQR will be checked for each features for outlier handling. ","metadata":{}},{"cell_type":"code","source":"# function to check upper IQR and lower IQR from columns\n\ndef find_outlier_boundary(df, variable):\n\n    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n\n    lower_boundary = df[variable].quantile(0.25) - (IQR * 1.5)\n    upper_boundary = df[variable].quantile(0.75) + (IQR * 1.5)\n\n    return upper_boundary, lower_boundary","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:13.523752Z","iopub.execute_input":"2024-06-21T07:56:13.524284Z","iopub.status.idle":"2024-06-21T07:56:13.533509Z","shell.execute_reply.started":"2024-06-21T07:56:13.524233Z","shell.execute_reply":"2024-06-21T07:56:13.531925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataframe to summarize IQR compared to minimum and maximum value for each columns\n\npd.DataFrame(data = {'Upper_IQR': [find_outlier_boundary(df_train_numerical, col)[0] for col in df_train_numerical.columns], \n                     'Maximum': [df_train_numerical[col].max() for col in df_train_numerical.columns], \n                     'Lower_IQR': [find_outlier_boundary(df_train_numerical, col)[1]  for col in df_train_numerical.columns], \n                     'Minimum': [df_train_numerical[col].min() for col in df_train_numerical.columns]}, \n             index = [col for col in df_train_numerical.columns])","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:13.535142Z","iopub.execute_input":"2024-06-21T07:56:13.535539Z","iopub.status.idle":"2024-06-21T07:56:13.632292Z","shell.execute_reply.started":"2024-06-21T07:56:13.535506Z","shell.execute_reply":"2024-06-21T07:56:13.631053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each values from features that is not in the range of Upper and Lower IQR will be replaced by their Upper and Lower IQR. ","metadata":{}},{"cell_type":"code","source":"# replace the outliers with upper and lower IQR\n\nfor col in df_train_numerical.columns:\n    Population_upper_limit, Population_lower_limit = find_outlier_boundary(df_train_numerical, col)\n    \n    df_train[col]= np.where(df_train[col] > Population_upper_limit, Population_upper_limit,\n                       np.where(df_train[col] < Population_lower_limit, Population_lower_limit, df_train[col]))","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:13.633574Z","iopub.execute_input":"2024-06-21T07:56:13.633943Z","iopub.status.idle":"2024-06-21T07:56:13.690940Z","shell.execute_reply.started":"2024-06-21T07:56:13.633911Z","shell.execute_reply":"2024-06-21T07:56:13.689892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot looping for distribution analysis after outlier treatment in features\n\nfor col in df_train_numerical.columns:\n    check_plot(df_train, col)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:13.692275Z","iopub.execute_input":"2024-06-21T07:56:13.692613Z","iopub.status.idle":"2024-06-21T07:56:26.535349Z","shell.execute_reply.started":"2024-06-21T07:56:13.692586Z","shell.execute_reply":"2024-06-21T07:56:26.533931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# features that have singular value after outlier handling will be dropped. \n\ndf_train_numerical = df_train_numerical.drop([ 'BsmtFinSF2', 'LowQualFinSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'MiscVal'], axis = 1)\ndf_train_numerical.columns","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:26.536765Z","iopub.execute_input":"2024-06-21T07:56:26.537155Z","iopub.status.idle":"2024-06-21T07:56:26.547871Z","shell.execute_reply.started":"2024-06-21T07:56:26.537124Z","shell.execute_reply":"2024-06-21T07:56:26.546505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Final features that will be used for modeling","metadata":{}},{"cell_type":"code","source":"final_features = list(df_train_categorical.columns) + list(df_train_numerical.columns) + list(df_train_discrete.columns)\nfinal_features","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:26.549335Z","iopub.execute_input":"2024-06-21T07:56:26.549746Z","iopub.status.idle":"2024-06-21T07:56:26.562034Z","shell.execute_reply.started":"2024-06-21T07:56:26.549701Z","shell.execute_reply":"2024-06-21T07:56:26.560639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Train and Test Assignment","metadata":{}},{"cell_type":"code","source":"# applying final features for model training\n\n# X_train and y_train\nX_train = df_train[final_features]\ny_train = df_train['SalePrice']\n\n#X_test and y_test\nX_test = df_test[final_features]\ny_test = df_test['SalePrice']","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:26.564266Z","iopub.execute_input":"2024-06-21T07:56:26.564830Z","iopub.status.idle":"2024-06-21T07:56:26.584173Z","shell.execute_reply.started":"2024-06-21T07:56:26.564786Z","shell.execute_reply":"2024-06-21T07:56:26.582867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:26.586271Z","iopub.execute_input":"2024-06-21T07:56:26.586816Z","iopub.status.idle":"2024-06-21T07:56:26.634358Z","shell.execute_reply.started":"2024-06-21T07:56:26.586772Z","shell.execute_reply":"2024-06-21T07:56:26.633044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pipeline for Feature Encoding and Feature Scaling","metadata":{}},{"cell_type":"code","source":"# Create Pipeline\n# target encoding will be used instead of One Hot Encoding technique to prevent curse of dimensionality\n\npreprocessor = Pipeline([\n    ('target', TargetEncoder(cols= list(df_train_categorical.columns))),\n    ('scaler', StandardScaler())\n])","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:26.636273Z","iopub.execute_input":"2024-06-21T07:56:26.636667Z","iopub.status.idle":"2024-06-21T07:56:26.643659Z","shell.execute_reply.started":"2024-06-21T07:56:26.636635Z","shell.execute_reply":"2024-06-21T07:56:26.642367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_processing = preprocessor.fit_transform(X_train, y_train)\n\nX_test_processing = preprocessor.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:26.645148Z","iopub.execute_input":"2024-06-21T07:56:26.645542Z","iopub.status.idle":"2024-06-21T07:56:26.905021Z","shell.execute_reply.started":"2024-06-21T07:56:26.645510Z","shell.execute_reply":"2024-06-21T07:56:26.903858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all features used in model training\n \npreprocessor.get_feature_names_out()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:26.906494Z","iopub.execute_input":"2024-06-21T07:56:26.906892Z","iopub.status.idle":"2024-06-21T07:56:26.914930Z","shell.execute_reply.started":"2024-06-21T07:56:26.906859Z","shell.execute_reply":"2024-06-21T07:56:26.913583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_processing = pd.DataFrame(X_train_processing,columns=preprocessor.get_feature_names_out())\nX_test_processing = pd.DataFrame(X_test_processing,columns=preprocessor.get_feature_names_out())","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:26.916706Z","iopub.execute_input":"2024-06-21T07:56:26.917226Z","iopub.status.idle":"2024-06-21T07:56:26.925984Z","shell.execute_reply.started":"2024-06-21T07:56:26.917193Z","shell.execute_reply":"2024-06-21T07:56:26.924511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Data","metadata":{}},{"cell_type":"code","source":"# baseline and ensemble model for training\n\nlinear = LinearRegression() #baseline model\nada = AdaBoostRegressor(random_state=42)\nrf = RandomForestRegressor(random_state=42)\nxgb = XGBRegressor(random_state=42)","metadata":{"id":"jMXtVJaRY6fG","execution":{"iopub.status.busy":"2024-06-21T07:56:26.927627Z","iopub.execute_input":"2024-06-21T07:56:26.928075Z","iopub.status.idle":"2024-06-21T07:56:26.940818Z","shell.execute_reply.started":"2024-06-21T07:56:26.928041Z","shell.execute_reply":"2024-06-21T07:56:26.939482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Selection","metadata":{}},{"cell_type":"code","source":"# model iteration\n# two metrics will be used: \n# 1. RMSE \n# 2. R2 (for result interpretation clarity only)\n\nml = [('Linear Regression',linear),\n      ('AdaBoost',ada),\n      ('Random Forest',rf),\n      ('XGBoost',xgb)]\n\nr2_train = []\nr2_test = []\n\nrmse_train = []\nrmse_test = []\n\nmodels = []\n\nfor model in ml:\n  model[1].fit(X_train_processing,y_train)\n\n  predict_train = model[1].predict(X_train_processing)\n  predict_test = model[1].predict(X_test_processing)\n\n  r2_score_train = r2_score(y_train,predict_train)\n  r2_score_test = r2_score(y_test,predict_test)\n\n  rmse_score_train = root_mean_squared_error(y_train,predict_train)\n  rmse_score_test = root_mean_squared_error(y_test,predict_test)\n\n  r2_train.append(r2_score_train)\n  r2_test.append(r2_score_test)\n\n  rmse_train.append(rmse_score_train)\n  rmse_test.append(rmse_score_test)\n\n  models.append(model[0])\n\ndf_metrics = pd.DataFrame({'model':models,\n                           'r2_score_train':r2_train,\n                           'r2_score_test':r2_test,\n                           'RMSE_train':rmse_train,\n                           'RMSE_test':rmse_test})","metadata":{"id":"vy6jCQyyZLrh","execution":{"iopub.status.busy":"2024-06-21T07:56:26.942509Z","iopub.execute_input":"2024-06-21T07:56:26.943001Z","iopub.status.idle":"2024-06-21T07:56:29.084355Z","shell.execute_reply.started":"2024-06-21T07:56:26.942959Z","shell.execute_reply":"2024-06-21T07:56:29.083355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_metrics","metadata":{"id":"sElSo1ercFfW","outputId":"f0f1a00b-e7d3-4c47-8d56-c9010895a8d1","execution":{"iopub.status.busy":"2024-06-21T07:56:29.090512Z","iopub.execute_input":"2024-06-21T07:56:29.094889Z","iopub.status.idle":"2024-06-21T07:56:29.109980Z","shell.execute_reply.started":"2024-06-21T07:56:29.094808Z","shell.execute_reply":"2024-06-21T07:56:29.107809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Linear Regression have similar value between high r2_score_train (0.834) and r2_score_test (0.832), which indicated no overfit and underfit. \n\n2. AdaBoost have similar value between high r2_score_train (0.853) and r2_score_test (0.849), which indicated no overfit and underfit. However, AdaBoost have higher R2 score and lower RMSE than Linear Regression. \n\n3. Random Forest have high difference between r2_score_train (0.977) and r2_score_test (0.873), which indicated this model is overfit. This model is categorized as overfit, indicating the model cannot understand general data pattern. \n\n4. XGBoost have extremely high difference between r2_score_train (0.999) and r2_score_test (0.891), which indicated this model is overfit. This model is categorized as overfit, indicating the model cannot understand general data pattern. \n\nAs summary, Linear Regression and AdaBoost have stable performance between evaluation metrics in data train and data test, because it has low RMSE values, while also not showing any overfit or underfit tendency. Linear Regression is also has potential to be used, but the model performance is still behind AdaBoost. Even though Random Forest and XGBoost have overfit, they shows potential in outperforming both Linear Regression and AdaBoost by having higher r2 and lower RMSE. To decide which model is the best, one model from the best stable performance (AdaBoost) and one model from the best overfit model will be tuned to determine which one is better.\n\nWe will do hyperparameter tuning to get the best parameter of AdaBoost and XGBoost. \n","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameter Tuning Parameter AdaBoost","metadata":{"id":"DDi4M32Oih1p"}},{"cell_type":"markdown","source":"We will do separate tuning for each hyperparameter first, then integrate the best range to Optuna later. \n\nWe will optimize log RMSE instead of RMSE, so the error in the small Sale Price will have more impact for model training. By doing this, the model will have better generalization in small Sale Price range. \n\nWe will choose hyperparameter that give the lowest log RMSE, since the objective is to reduce the error as small as possible. ","metadata":{}},{"cell_type":"markdown","source":"### Learning Rate Tuning","metadata":{}},{"cell_type":"code","source":"learning_rate_list = [0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n\nlearning_rate = []\nrmse_score = []\n\n\nfor eta in learning_rate_list:\n  ada = AdaBoostRegressor(learning_rate=eta, random_state=42).fit(X_train_processing,y_train)\n  rmse = root_mean_squared_error(np.log(y_test), np.log(ada.predict(X_test_processing)))\n  rmse_score.append(rmse)\n  learning_rate.append(eta)\n\n\ndf_learning_rate = pd.DataFrame({'learning_rate':learning_rate,\n                       'rmse_score':rmse_score})\n\nplt.figure(figsize=(8,6))\nsns.pointplot(data=df_learning_rate,x='learning_rate',y='rmse_score')\nplt.title('Tuning Learning Rate AdaBoost')\nplt.show()","metadata":{"id":"EOyuFvz7hnxk","execution":{"iopub.status.busy":"2024-06-21T07:56:29.112914Z","iopub.execute_input":"2024-06-21T07:56:29.113552Z","iopub.status.idle":"2024-06-21T07:56:33.213451Z","shell.execute_reply.started":"2024-06-21T07:56:29.113505Z","shell.execute_reply":"2024-06-21T07:56:33.212062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Optimal learning rate = 0.8","metadata":{"id":"T5tjTkyVrQs5"}},{"cell_type":"markdown","source":"### n_estimators Tuning","metadata":{}},{"cell_type":"code","source":"n_estimators = []\nrmse_score = []\n\nfor estimator in [10, 25, 50, 75, 100, 250]:\n  ada = AdaBoostRegressor(n_estimators=estimator, random_state=42).fit(X_train_processing,y_train)\n  rmse = root_mean_squared_error(np.log(y_test), np.log(ada.predict(X_test_processing)))\n  rmse_score.append(rmse)\n  n_estimators.append(estimator)\n\n\ndf_estimator = pd.DataFrame({'n_estimators':n_estimators,\n                       'rmse_score':rmse_score})\n\nplt.figure(figsize=(8,6))\nsns.pointplot(data=df_estimator,x='n_estimators',y='rmse_score')\nplt.title('Tuning n_estimators AdaBoost')\nplt.show()","metadata":{"id":"kf2A79dRpLA-","outputId":"5f30ee7a-30ce-425d-bdd5-019c2490c057","execution":{"iopub.status.busy":"2024-06-21T07:56:33.215500Z","iopub.execute_input":"2024-06-21T07:56:33.216042Z","iopub.status.idle":"2024-06-21T07:56:36.172788Z","shell.execute_reply.started":"2024-06-21T07:56:33.215992Z","shell.execute_reply":"2024-06-21T07:56:36.171413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Optimal n_estimators = 25","metadata":{"id":"oJbjj_lmsK1V"}},{"cell_type":"markdown","source":"### Loss Tuning","metadata":{}},{"cell_type":"code","source":"loss = []\nrmse_score = []\n\nfor cat in ['linear', 'square', 'exponential']:\n  ada = AdaBoostRegressor(loss=cat, random_state = 42).fit(X_train_processing,y_train)\n  rmse = root_mean_squared_error(np.log(y_test), np.log(ada.predict(X_test_processing)))\n  rmse_score.append(rmse)\n  loss.append(cat)\n\n\ndf_loss = pd.DataFrame({'loss':loss,\n                       'rmse_score':rmse_score})\n\nplt.figure(figsize=(8,6))\nsns.pointplot(data=df_loss,x='loss',y='rmse_score')\nplt.title('Tuning loss AdaBoost')\nplt.show()","metadata":{"id":"SDoRItlBqQWx","outputId":"e0cb0651-597e-48f6-bfbd-76ed9a949f72","execution":{"iopub.status.busy":"2024-06-21T07:56:36.174643Z","iopub.execute_input":"2024-06-21T07:56:36.175064Z","iopub.status.idle":"2024-06-21T07:56:37.326610Z","shell.execute_reply.started":"2024-06-21T07:56:36.175031Z","shell.execute_reply":"2024-06-21T07:56:37.325342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Optimal loss  = exponential","metadata":{"id":"zihw2YQ0sk88"}},{"cell_type":"markdown","source":"### Model Performance Comparison with and without Tuning (AdaBoost)","metadata":{"id":"_GCvEzUzul4s"}},{"cell_type":"markdown","source":"#### Without Tuning","metadata":{}},{"cell_type":"code","source":"ada = AdaBoostRegressor(random_state=42)\nada.fit(X_train_processing,y_train)\n\n\npredict_train_without_tuning = ada.predict(X_train_processing)\npredict_test_without_tuning = ada.predict(X_test_processing)\n\nr2_score_train = r2_score(np.log(y_train), np.log(predict_train_without_tuning))\nr2_score_test = r2_score(np.log(y_test), np.log(predict_test_without_tuning))\n\nrmse_score_train = root_mean_squared_error(np.log(y_train), np.log(predict_train_without_tuning))\nrmse_score_test = root_mean_squared_error(np.log(y_test), np.log(predict_test_without_tuning))\n\nprint('r2 score train',r2_score_train)\nprint('r2 score test',r2_score_test)\n\nprint('RMSE train',rmse_score_train)\nprint('RMSE test',rmse_score_test)","metadata":{"id":"DCNazSmKvR-Z","outputId":"4ae72d8f-79f9-47cc-c9cf-60b63b639287","execution":{"iopub.status.busy":"2024-06-21T07:56:37.328279Z","iopub.execute_input":"2024-06-21T07:56:37.328727Z","iopub.status.idle":"2024-06-21T07:56:37.645873Z","shell.execute_reply.started":"2024-06-21T07:56:37.328684Z","shell.execute_reply":"2024-06-21T07:56:37.644485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r2_score_train_without_tuning = r2_score_train\nr2_score_test_without_tuning = r2_score_test\nrmse_score_train_without_tuning = rmse_score_train\nrmse_score_test_without_tuning = rmse_score_test","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:37.647404Z","iopub.execute_input":"2024-06-21T07:56:37.647756Z","iopub.status.idle":"2024-06-21T07:56:37.653384Z","shell.execute_reply.started":"2024-06-21T07:56:37.647718Z","shell.execute_reply":"2024-06-21T07:56:37.652125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### With Tuning","metadata":{}},{"cell_type":"code","source":"ada_tuning = AdaBoostRegressor(learning_rate = 0.8,\n                  n_estimators = 25,\n                  loss = 'exponential',\n                  random_state=42)\n\nada_tuning.fit(X_train_processing,y_train)\n\npredict_train_tuning = ada_tuning.predict(X_train_processing)\npredict_test_tuning = ada_tuning.predict(X_test_processing)\n\nr2_score_train = r2_score(np.log(y_train), np.log(predict_train_tuning))\nr2_score_test = r2_score(np.log(y_test), np.log(predict_test_tuning))\n\nrmse_score_train = root_mean_squared_error(np.log(y_train), np.log(predict_train_tuning))\nrmse_score_test = root_mean_squared_error(np.log(y_test), np.log(predict_test_tuning))\n\nprint('r2 score train',r2_score_train)\nprint('r2 score test',r2_score_test)\n\nprint('RMSE train',rmse_score_train)\nprint('RMSE test',rmse_score_test)","metadata":{"id":"WtvsGUGUuaKa","outputId":"27b4655d-7303-44e6-9d38-7bcdd7ac3929","execution":{"iopub.status.busy":"2024-06-21T07:56:37.655351Z","iopub.execute_input":"2024-06-21T07:56:37.655760Z","iopub.status.idle":"2024-06-21T07:56:37.839688Z","shell.execute_reply.started":"2024-06-21T07:56:37.655698Z","shell.execute_reply":"2024-06-21T07:56:37.838343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best parameters: learning_rate = 0.8,\n                  n_estimators = 25,\n                  loss = 'exponential',\n                  random_state=42.","metadata":{}},{"cell_type":"code","source":"r2_score_train_tuning = r2_score_train\nr2_score_test_tuning = r2_score_test\nrmse_score_train_tuning = rmse_score_train\nrmse_score_test_tuning = rmse_score_test","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:37.841305Z","iopub.execute_input":"2024-06-21T07:56:37.841694Z","iopub.status.idle":"2024-06-21T07:56:37.848339Z","shell.execute_reply.started":"2024-06-21T07:56:37.841661Z","shell.execute_reply":"2024-06-21T07:56:37.846486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will compare the results from AdaBoost optimization with XGBoost later. ","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameter Tuning Parameter XGBoost","metadata":{}},{"cell_type":"markdown","source":"### Learning Rate Tuning","metadata":{}},{"cell_type":"code","source":"learning_rate_list = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n\nlearning_rate = []\nrmse_score = []\n\n\nfor eta in learning_rate_list:\n  xgb = XGBRegressor(learning_rate=eta,random_state=42).fit(X_train_processing, y_train)\n  rmse = root_mean_squared_error(np.log(y_test), np.log(xgb.predict(X_test_processing)))\n\n  rmse_score.append(rmse)\n  learning_rate.append(eta)\n\n\ndf_eta = pd.DataFrame({'learning_rate':learning_rate,\n                       'rmse_score':rmse_score})\n\nplt.figure(figsize=(8,6))\nsns.pointplot(data=df_eta,x='learning_rate',y='rmse_score')\nplt.title('Tuning Learning Rate XGBoost')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:37.849769Z","iopub.execute_input":"2024-06-21T07:56:37.850207Z","iopub.status.idle":"2024-06-21T07:56:41.555494Z","shell.execute_reply.started":"2024-06-21T07:56:37.850173Z","shell.execute_reply":"2024-06-21T07:56:41.553997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Optimal learning rate: 0.1","metadata":{}},{"cell_type":"markdown","source":"### Max_depth Tuning","metadata":{}},{"cell_type":"code","source":"max_depth = []\nrmse_score = []\n\nfor depth in range(1,13):\n  xgb = XGBRegressor(max_depth=depth, random_state=42).fit(X_train_processing, y_train)\n  rmse = root_mean_squared_error(np.log(y_test), np.log(xgb.predict(X_test_processing)))\n  rmse_score.append(rmse)\n  max_depth.append(depth)\n\n\ndf_max_depth = pd.DataFrame({'max_depth':max_depth,\n                       'rmse_score':rmse_score})\n\nplt.figure(figsize=(8,6))\nsns.pointplot(data=df_max_depth,x='max_depth',y='rmse_score')\nplt.title('Tuning max_depth XGBoost')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:41.557615Z","iopub.execute_input":"2024-06-21T07:56:41.558044Z","iopub.status.idle":"2024-06-21T07:56:46.324024Z","shell.execute_reply.started":"2024-06-21T07:56:41.558010Z","shell.execute_reply":"2024-06-21T07:56:46.322647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Optimal max_depth: 2","metadata":{}},{"cell_type":"markdown","source":"### N_estimator Tuning","metadata":{}},{"cell_type":"code","source":"n_estimators = []\nrmse_score = []\n\nfor estimator in [50, 75, 100, 125, 150, 175, 200]:\n  xgb = XGBRegressor(n_estimators=estimator, random_state = 42).fit(X_train_processing, y_train)\n  rmse = root_mean_squared_error(np.log(y_test), np.log(xgb.predict(X_test_processing)))\n  rmse_score.append(rmse)\n  n_estimators.append(estimator)\n\n\ndf_estimator = pd.DataFrame({'n_estimators':n_estimators,\n                       'rmse_score':rmse_score})\n\nplt.figure(figsize=(8,6))\nsns.pointplot(data=df_estimator,x='n_estimators',y='rmse_score')\nplt.title('Tuning n_estimators XGBoost')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:46.325434Z","iopub.execute_input":"2024-06-21T07:56:46.325785Z","iopub.status.idle":"2024-06-21T07:56:48.636642Z","shell.execute_reply.started":"2024-06-21T07:56:46.325755Z","shell.execute_reply":"2024-06-21T07:56:48.635257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Optimal n_estimators = 125","metadata":{}},{"cell_type":"markdown","source":"### Subsample Tuning","metadata":{}},{"cell_type":"code","source":"subsample = []\nrmse_score = []\n\nfor sub in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]:\n  xgb = XGBRegressor(subsample=sub,random_state=42).fit(X_train_processing, y_train)\n  rmse = root_mean_squared_error(np.log(y_test), np.log(xgb.predict(X_test_processing)))\n  rmse_score.append(rmse)\n  subsample.append(sub)\n\n\ndf_subsample = pd.DataFrame({'subsample':subsample,\n                       'rmse_score':rmse_score})\n\nplt.figure(figsize=(8,6))\nsns.pointplot(data=df_subsample,x='subsample',y='rmse_score')\nplt.title('Tuning subsample XGBoost')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:48.638335Z","iopub.execute_input":"2024-06-21T07:56:48.638763Z","iopub.status.idle":"2024-06-21T07:56:51.199199Z","shell.execute_reply.started":"2024-06-21T07:56:48.638724Z","shell.execute_reply":"2024-06-21T07:56:51.198031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Optimal subsample: 0.9","metadata":{}},{"cell_type":"markdown","source":"### Lambda Tuning","metadata":{}},{"cell_type":"code","source":"reg_lambda = []\nrmse_score = []\n\nfor lambda_ in [0.0001,0.001, 0.01, 0.1, 0.5, 1, 1.5, 1.6, 1.7, 1.8, 1.9, 2, 2.2, 2.5, 3]:\n    xgb = XGBRegressor(reg_lambda=lambda_, random_state=42).fit(X_train_processing, y_train)\n    rmse = root_mean_squared_error(np.log(y_test), np.log(xgb.predict(X_test_processing)))\n    rmse_score.append(rmse)\n    reg_lambda.append(lambda_)\n\n\n#create dataset eta\ndf_reg_lambda = pd.DataFrame({'reg_lambda':reg_lambda,\n                       'rmse_score':rmse_score})\n\nplt.figure(figsize=(8,6))\nsns.pointplot(data=df_reg_lambda,x='reg_lambda',y='rmse_score')\nplt.title('Tuning reg_lambda XGBoost')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:51.200816Z","iopub.execute_input":"2024-06-21T07:56:51.201299Z","iopub.status.idle":"2024-06-21T07:56:55.051205Z","shell.execute_reply.started":"2024-06-21T07:56:51.201259Z","shell.execute_reply":"2024-06-21T07:56:55.049937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Optimal lambda = 1.6","metadata":{}},{"cell_type":"markdown","source":"### Model Performance Comparison with and without Tuning (XGBoost)","metadata":{}},{"cell_type":"markdown","source":"#### Without Tuning","metadata":{}},{"cell_type":"code","source":"xgb = XGBRegressor(random_state=42)\nxgb.fit(X_train_processing,y_train)\n\n\npredict_train_without_tuning = xgb.predict(X_train_processing)\npredict_test_without_tuning = xgb.predict(X_test_processing)\n\nr2_score_train = r2_score(np.log(y_train), np.log(predict_train_without_tuning))\nr2_score_test = r2_score(np.log(y_test), np.log(predict_test_without_tuning))\n\nrmse_score_train = root_mean_squared_error(np.log(y_train), np.log(predict_train_without_tuning))\nrmse_score_test = root_mean_squared_error(np.log(y_test), np.log(predict_test_without_tuning))\n\nprint('r2 score train',r2_score_train)\nprint('r2 score test',r2_score_test)\n\nprint('RMSE train',rmse_score_train)\nprint('RMSE test',rmse_score_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:55.063446Z","iopub.execute_input":"2024-06-21T07:56:55.063912Z","iopub.status.idle":"2024-06-21T07:56:55.319231Z","shell.execute_reply.started":"2024-06-21T07:56:55.063873Z","shell.execute_reply":"2024-06-21T07:56:55.318137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r2_score_train_without_tuning_xgb = r2_score_train\nr2_score_test_without_tuning_xgb = r2_score_test\nrmse_score_train_without_tuning_xgb = rmse_score_train\nrmse_score_test_without_tuning_xgb = rmse_score_test","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:55.324007Z","iopub.execute_input":"2024-06-21T07:56:55.325181Z","iopub.status.idle":"2024-06-21T07:56:55.330025Z","shell.execute_reply.started":"2024-06-21T07:56:55.325139Z","shell.execute_reply":"2024-06-21T07:56:55.328934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### With Tuning","metadata":{}},{"cell_type":"code","source":"xgb_tuning = XGBRegressor(learning_rate = 0.1,\n                  max_depth = 2,\n                  n_estimators = 125,\n                  subsample = 0.9,\n                  reg_lambda = 1.6,\n                  random_state=42)\n\nxgb_tuning.fit(X_train_processing, y_train)\n\npredict_train_tuning = xgb_tuning.predict(X_train_processing)\npredict_test_tuning = xgb_tuning.predict(X_test_processing)\n\nr2_score_train = r2_score(np.log(y_train), np.log(predict_train_tuning))\nr2_score_test = r2_score(np.log(y_test), np.log(predict_test_tuning))\n\nrmse_score_train = root_mean_squared_error(np.log(y_train), np.log(predict_train_tuning))\nrmse_score_test = root_mean_squared_error(np.log(y_test), np.log(predict_test_tuning))\n\nprint('r2 score train',r2_score_train)\nprint('r2 score test',r2_score_test)\n\nprint('RMSE train',rmse_score_train)\nprint('RMSE test',rmse_score_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:55.331042Z","iopub.execute_input":"2024-06-21T07:56:55.331360Z","iopub.status.idle":"2024-06-21T07:56:55.476662Z","shell.execute_reply.started":"2024-06-21T07:56:55.331334Z","shell.execute_reply":"2024-06-21T07:56:55.475586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best parameters: learning_rate = 0.1,\n                  max_depth = 2,\n                  n_estimators = 125,\n                  subsample = 0.9,\n                  reg_lambda = 1.6,\n                  random_state=42.","metadata":{}},{"cell_type":"code","source":"r2_score_train_tuning_xgb = r2_score_train\nr2_score_test_tuning_xgb = r2_score_test\nrmse_score_train_tuning_xgb = rmse_score_train\nrmse_score_test_tuning_xgb = rmse_score_test","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:55.481231Z","iopub.execute_input":"2024-06-21T07:56:55.482497Z","iopub.status.idle":"2024-06-21T07:56:55.487523Z","shell.execute_reply.started":"2024-06-21T07:56:55.482459Z","shell.execute_reply":"2024-06-21T07:56:55.486329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Comparison Between AdaBoost and XGBoost Performance Before and After Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"index = ['AdaBoost R2 without Tuning', 'AdaBoost R2 with Tuning', 'XGBoost R2 without Tuning', 'XGBoost R2 with Tuning', \n         'AdaBoost RMSE without Tuning', 'AdaBoost RMSE with Tuning', 'XGBoost RMSE test without Tuning', 'XGBoost RMSE test with Tuning']\ndf_compare = pd.DataFrame({\n    'Train': [r2_score_train_without_tuning, r2_score_train_tuning, r2_score_train_without_tuning_xgb, r2_score_train_tuning_xgb, \n              rmse_score_train_without_tuning, rmse_score_train_tuning, rmse_score_train_without_tuning_xgb, rmse_score_train_tuning_xgb],\n    'Test': [r2_score_test_without_tuning, r2_score_test_tuning, r2_score_test_without_tuning_xgb, r2_score_test_tuning_xgb, \n              rmse_score_test_without_tuning, rmse_score_test_tuning, rmse_score_test_without_tuning_xgb, rmse_score_test_tuning_xgb]\n}, index=index)\n\ndf_compare['Difference'] = abs(df_compare['Test'] - df_compare['Train'])\n\nprint('\\nR2 and RMSE comparison without tuning and with tuning:')\ndf_compare","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:55.488752Z","iopub.execute_input":"2024-06-21T07:56:55.489147Z","iopub.status.idle":"2024-06-21T07:56:55.514181Z","shell.execute_reply.started":"2024-06-21T07:56:55.489115Z","shell.execute_reply":"2024-06-21T07:56:55.512870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While AdaBoost provides robust and stable performance before and after tuning, XGBoost give significant improvement. At first XGBoost have extremely overfit before tuning. After tuning, the RMSE difference XGBoost is reduced to 0.04 from 0.15. \n\nAfter tuning, XGBoost has higher R2 score and lower RMSE than AdaBoost, while also improved from overfit to be an optimal model. Hence, XGBoost is the best model among the other models, and XGBoost will be further improved through hyperparameter tuning with Optuna. ","metadata":{}},{"cell_type":"markdown","source":"## Hyperparameter Tuning XGBoost using Optuna","metadata":{}},{"cell_type":"markdown","source":"The range of best parameters from before will be used in Optuna. ","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    param = {\n        'objective': 'reg:squarederror',\n        'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.3),\n        'max_depth': trial.suggest_int('max_depth', 2, 3),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 175),\n        'subsample': trial.suggest_float('subsample', 0.8, 1),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1.5, 1.7),\n    }\n\n    model = XGBRegressor(**param,random_state=42)\n    model.fit(X_train_processing, y_train, eval_set=[(X_test_processing, y_test)], early_stopping_rounds=50, verbose=False)\n\n    # RMSE calculation\n    preds = model.predict(X_test_processing)\n    rmse = root_mean_squared_error(np.log(y_test), np.log(preds))\n    \n    return rmse\n\nstudy = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))  \nstudy.optimize(objective, n_trials=100)\n\nprint('Best trial:', study.best_trial.params)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:56:55.515492Z","iopub.execute_input":"2024-06-21T07:56:55.515990Z","iopub.status.idle":"2024-06-21T07:57:22.246107Z","shell.execute_reply.started":"2024-06-21T07:56:55.515946Z","shell.execute_reply":"2024-06-21T07:57:22.245053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = study.best_trial.params\nbest_params","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:57:22.251262Z","iopub.execute_input":"2024-06-21T07:57:22.251657Z","iopub.status.idle":"2024-06-21T07:57:22.259535Z","shell.execute_reply.started":"2024-06-21T07:57:22.251627Z","shell.execute_reply":"2024-06-21T07:57:22.258215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hyperparameter importance from Optuna\n\nvis.plot_param_importances(study)","metadata":{"id":"jDC_ACm1fps-","outputId":"d2495225-3b39-496c-fdf9-e5beed417016","execution":{"iopub.status.busy":"2024-06-21T07:57:22.260908Z","iopub.execute_input":"2024-06-21T07:57:22.261261Z","iopub.status.idle":"2024-06-21T07:57:24.239049Z","shell.execute_reply.started":"2024-06-21T07:57:22.261232Z","shell.execute_reply":"2024-06-21T07:57:24.237906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The importance of max_depth learning is the highest, which indicated this as the high importance feature that needs to be tuned. ","metadata":{}},{"cell_type":"code","source":"xgb_optuna = XGBRegressor(**best_params,\n                  random_state=42)\n\nxgb_optuna.fit(X_train_processing,y_train)\n\npredict_train_optuna = xgb_optuna.predict(X_train_processing)\npredict_test_optuna = xgb_optuna.predict(X_test_processing)\n\nr2_score_train = r2_score(np.log(y_train), np.log(predict_train_optuna))\nr2_score_test = r2_score(np.log(y_test), np.log(predict_test_optuna))\n\nrmse_score_train = root_mean_squared_error(np.log(y_train), np.log(predict_train_optuna))\nrmse_score_test = root_mean_squared_error(np.log(y_test), np.log(predict_test_optuna))\n\nprint('r2 score train', r2_score_train)\nprint('r2 score test', r2_score_test)\n\nprint('RMSE train', rmse_score_train)\nprint('RMSE test', rmse_score_test)","metadata":{"id":"C_47kkgNdiZz","outputId":"65696b89-e998-40c7-d5d0-6a2cb6f0fdbb","execution":{"iopub.status.busy":"2024-06-21T07:57:24.241002Z","iopub.execute_input":"2024-06-21T07:57:24.241433Z","iopub.status.idle":"2024-06-21T07:57:24.414626Z","shell.execute_reply.started":"2024-06-21T07:57:24.241394Z","shell.execute_reply":"2024-06-21T07:57:24.413699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r2_score_train_optuna = r2_score_train\nr2_score_test_optuna = r2_score_test\nrmse_score_train_optuna = rmse_score_train\nrmse_score_test_optuna = rmse_score_test","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:57:24.416184Z","iopub.execute_input":"2024-06-21T07:57:24.416949Z","iopub.status.idle":"2024-06-21T07:57:24.421786Z","shell.execute_reply.started":"2024-06-21T07:57:24.416913Z","shell.execute_reply":"2024-06-21T07:57:24.420856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Comparison of Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"index = ['R2 without Tuning', 'R2 with Tuning', 'R2 Optima', 'RMSE without Tuning', 'RMSE Tuning', 'RMSE Optuna']\ndf_compare = pd.DataFrame({\n    'Train': [r2_score_train_without_tuning_xgb, r2_score_train_tuning_xgb, r2_score_train_optuna, rmse_score_train_without_tuning_xgb, rmse_score_train_tuning_xgb, rmse_score_train_optuna],\n    'Test': [r2_score_test_without_tuning_xgb, r2_score_test_tuning_xgb, r2_score_test_optuna, rmse_score_test_without_tuning_xgb, rmse_score_test_tuning_xgb, rmse_score_test_optuna]\n}, index=index)\n\ndf_compare['Difference'] = abs(df_compare['Test'] - df_compare['Train'])\n\nprint('\\nR2 and RMSE comparison without tuning, with tuning, and tuning with Optuna:')\ndf_compare","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:57:24.423413Z","iopub.execute_input":"2024-06-21T07:57:24.424131Z","iopub.status.idle":"2024-06-21T07:57:24.443664Z","shell.execute_reply.started":"2024-06-21T07:57:24.424097Z","shell.execute_reply":"2024-06-21T07:57:24.442644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualization for Test Dataset Prediction using Optuna","metadata":{}},{"cell_type":"code","source":"# actual vs predicted house sale prices in test dataset\nfinal_result = pd.DataFrame({'actual': y_test, 'predicted': predict_test_optuna})\nfinal_result.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:57:24.445297Z","iopub.execute_input":"2024-06-21T07:57:24.446048Z","iopub.status.idle":"2024-06-21T07:57:24.459234Z","shell.execute_reply.started":"2024-06-21T07:57:24.446013Z","shell.execute_reply":"2024-06-21T07:57:24.458083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.scatterplot(x=final_result['actual'], y=final_result['predicted'])\nplt.plot([final_result['actual'].min(), final_result['actual'].max()], [final_result['actual'].min(), final_result['actual'].max()], 'r--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Actual vs Predicted House Sale Prices')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:57:24.460370Z","iopub.execute_input":"2024-06-21T07:57:24.460698Z","iopub.status.idle":"2024-06-21T07:57:24.872723Z","shell.execute_reply.started":"2024-06-21T07:57:24.460671Z","shell.execute_reply":"2024-06-21T07:57:24.871577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, almost all predictions is near the straight line, which is a good sign the model predicted well. ","metadata":{}},{"cell_type":"markdown","source":"## Summary\n\nInitially, XGBoost model performance is overfit, then improved significantly after hyperparameter tuning. Both RMSE train and test are decreased after tuning. The difference between R2 score and RMSE train and test also become smaller after hyperparameter tuning, with the best performance from hyperparameter tuning with Optuna. \n\nAfter comparing R2 and RMSE values, we will use model with Optuna tuning as the final best model, since it give the lowest RMSE value, the highest R2, while also give the smallest difference between train and test. The best parameters that will be used for house price prediction are based on Optuna tuning: ","metadata":{}},{"cell_type":"code","source":"print('Best Hyperparameters:')\nbest_params","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:57:24.874348Z","iopub.execute_input":"2024-06-21T07:57:24.874784Z","iopub.status.idle":"2024-06-21T07:57:24.883455Z","shell.execute_reply.started":"2024-06-21T07:57:24.874745Z","shell.execute_reply":"2024-06-21T07:57:24.882299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Use Data Test for House Prediction","metadata":{}},{"cell_type":"code","source":"# read data test\n\ndata_test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\nids = data_test.pop('Id')","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:57:24.885001Z","iopub.execute_input":"2024-06-21T07:57:24.885356Z","iopub.status.idle":"2024-06-21T07:57:24.922987Z","shell.execute_reply.started":"2024-06-21T07:57:24.885327Z","shell.execute_reply":"2024-06-21T07:57:24.921886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reassign variables for data test prediction\n\ncategorical_test = [col for col in df_train.columns if df[col].dtype not in ['float64','int64']]\ndiscrete_test = [col for col in df_train.columns if df[col].nunique() < 20 and col not in categorical_test]\nnumerical_test = [col for col in df_train.columns if col not in categorical_test + discrete_test + ['SalePrice']]","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:57:24.924485Z","iopub.execute_input":"2024-06-21T07:57:24.924885Z","iopub.status.idle":"2024-06-21T07:57:24.944165Z","shell.execute_reply.started":"2024-06-21T07:57:24.924827Z","shell.execute_reply":"2024-06-21T07:57:24.942910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# impute data test based on train dataset information\n\ndata_test[numerical_test] = impute_numerical.transform(data_test[numerical_test])\ndata_test[discrete_test] = impute_discrete.transform(data_test[discrete_test])\ndata_test[categorical_test] = impute_categorical.transform(data_test[categorical_test])","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:57:24.945691Z","iopub.execute_input":"2024-06-21T07:57:24.946080Z","iopub.status.idle":"2024-06-21T07:57:24.979345Z","shell.execute_reply.started":"2024-06-21T07:57:24.946041Z","shell.execute_reply":"2024-06-21T07:57:24.978236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use the same column as train\n\ndata_test = data_test[final_features]","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:57:24.980575Z","iopub.execute_input":"2024-06-21T07:57:24.980956Z","iopub.status.idle":"2024-06-21T07:57:24.990279Z","shell.execute_reply.started":"2024-06-21T07:57:24.980925Z","shell.execute_reply":"2024-06-21T07:57:24.988933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocessing data test based on train dataset information\n\ndata_test_prediction = preprocessor.transform(data_test)\ndata_test_prediction = pd.DataFrame(data_test_prediction,columns=preprocessor.get_feature_names_out())","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:57:24.992455Z","iopub.execute_input":"2024-06-21T07:57:24.992857Z","iopub.status.idle":"2024-06-21T07:57:25.045517Z","shell.execute_reply.started":"2024-06-21T07:57:24.992812Z","shell.execute_reply":"2024-06-21T07:57:25.044420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict data test using the best model \n\npred = xgb_optuna.predict(data_test_prediction) ","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:57:25.046872Z","iopub.execute_input":"2024-06-21T07:57:25.047250Z","iopub.status.idle":"2024-06-21T07:57:25.061331Z","shell.execute_reply.started":"2024-06-21T07:57:25.047219Z","shell.execute_reply":"2024-06-21T07:57:25.060350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input prediction result to dataframe for submission\n\ndf_submit = pd.DataFrame(pred, index=ids, columns=['SalePrice']).reset_index()\ndf_submit.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:57:25.062383Z","iopub.execute_input":"2024-06-21T07:57:25.063466Z","iopub.status.idle":"2024-06-21T07:57:25.087083Z","shell.execute_reply.started":"2024-06-21T07:57:25.063432Z","shell.execute_reply":"2024-06-21T07:57:25.085853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save prediction to sample_submission.csv\n\ndf_submission = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv')\ndf_submission['SalePrice'] = df_submit['SalePrice']\ndf_submission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T07:57:25.089245Z","iopub.execute_input":"2024-06-21T07:57:25.089614Z","iopub.status.idle":"2024-06-21T07:57:25.106537Z","shell.execute_reply.started":"2024-06-21T07:57:25.089583Z","shell.execute_reply":"2024-06-21T07:57:25.105245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thank you for exploring my notebook! I hope you found it informative and that it enriched your knowledge. If you have any questions, suggestions, or feedback, please do not hesitate to reach out. I genuinely value your input! \n\nIf you found this notebook helpful, I would greatly appreciate an upvote. Your support encourages me to continue creating more high-quality content! ","metadata":{}}]}